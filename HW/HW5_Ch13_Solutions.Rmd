---
title: "HW 5 Solutions"
author: "Shravan Vasishth"
date: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("index.R")
```

In these exercises, you can use any or all of the four methods discussed in the lecture. In general, bridgesampling will be time-consuming; Savage-Dickey will be faster.

# Exercise 1: Is there evidence for differences in the effect of cloze probability among the subjects?

 Use Bayes factor to compare the centered cloze probability model,  with a similar model but one that incorporates the strong assumption of no difference between subjects for the effect of centered cloze ($\tau_{u_2}=0$).
 
## Solution

```{r}
data("df_eeg")
head(df_eeg)

data(df_eeg)
df_eeg <- df_eeg %>%
  mutate(c_cloze = cloze - mean(cloze))
```

```{r}
priorsFULL <-
  c(prior(normal(0, 10), class = Intercept),
    prior(normal(0, 10), class = b, coef = c_cloze),
    prior(normal(0, 50), class = sigma),
    prior(normal(0, 20), class = sd, coef = Intercept, group = subj),
    prior(normal(0, 5), class = sd, coef = c_cloze, group = subj),
    prior(normal(0, 20), class = sd, coef = Intercept, group = item),
    prior(normal(0, 5), class = sd, coef = c_cloze, group = item),
     prior(lkj(2), class = cor))

priorsNULL <-
  c(prior(normal(0, 10), class = Intercept),
    prior(normal(0, 10), class = b, coef = c_cloze),
    prior(normal(0, 50), class = sigma),
    prior(normal(0, 20), class = sd, coef = Intercept, group = subj),
    #prior(normal(0, 5), class = sd, coef = c_cloze, group = subj),
    prior(normal(0, 20), class = sd, coef = Intercept, group = item),
    prior(normal(0, 5), class = sd, coef = c_cloze, group = item),
     prior(lkj(2), class = cor, group = item))
```

Using bridge sampling:

```{r message = FALSE, results = "hide", cache=TRUE}
fit_N400_h <- brm(n400 ~ c_cloze +
                        (c_cloze | subj) + (c_cloze | item),
                      prior = priorsFULL,
                      warmup = 2000,
                      iter = 20000,
                      control = list(adapt_delta = 0.9),
                      save_pars = save_pars(all = TRUE),
                      data = df_eeg)

fit_N400_h_NULL <- brm(n400 ~ c_cloze +
                        (1 | subj) + (c_cloze | item),
                      prior = priorsNULL,
                      warmup = 2000,
                      iter = 20000,
                      control = list(adapt_delta = 0.9),
                      save_pars = save_pars(all = TRUE),
                      data = df_eeg)
```

```{r cache=TRUE}
bayes_factor(fit_N400_h,fit_N400_h_NULL)

## check for stability:
bayes_factor(fit_N400_h,fit_N400_h_NULL)
```

Using Savage-Dickey

```{r}
## Simulate prior:
samp<-extraDistr::rtnorm(10000,mean=0,sd=5,a=0)
hist(samp,freq=FALSE)
## twice that of the untruncated normal because
## the truncated normal has to be renormalized:
prior_dens_0<-dnorm(0,mean=0,sd=5)*2

plot(density(as_draws_df(fit_N400_h)$sd_subj__c_cloze))
post_dens_0<-0.325 ## eyeballed from histogram

## evidence for full model:
prior_dens_0/post_dens_0
```

Conclusion: No evidence for between-subject variability in c_cloze. But the result from the Bayes factor is **inconclusive**. We can't say that there is evidence against between-subject variability in c_cloze. Be careful about wording and conclusions.

Compare with lmer:

```{r}
library(lme4)
mFULL<-lmer(n400 ~ c_cloze +
                        (1 + c_cloze || subj) + (c_cloze | item),
                      data = df_eeg)
mNULL<-lmer(n400 ~ c_cloze +
                        (1 | subj) + (c_cloze | item),
                      data = df_eeg)

anova(mFULL,mNULL)
```

It would be a mistake to conclude that there is evidence against between subject variability in c_cloze.

# Exercise 2: Is there evidence for the claim that English subject relative clauses are easier to process than object relative clauses?


Consider again the reading time data coming from Experiment 1 of @grodner presented in exercise \@ref(exr:hierarchical-logn):

```{r open_grodneretal_again, message = FALSE}
data("df_gg05_rc")
df_gg05_rc
```

You should use a sum coding for the predictors. Here, object relative clauses (`"objgaps"`) are coded $+1/2$, and subject relative clauses as $-1/2$.

```{r}
df_gg05_rc <- df_gg05_rc %>%
  mutate(c_cond = if_else(condition == "objgap", 1/2, -1/2))
head(df_gg05_rc)
```


Using the bayes_factor function discussed in class, quantify the evidence against the null model (no population-level reading time difference between SRC and ORC) relative to the following alternative models:

a. $\beta \sim \mathit{Normal}(0, 1)$
b. $\beta \sim \mathit{Normal}(0, 0.1)$
c. $\beta \sim \mathit{Normal}(0, 0.01)$
d. $\beta \sim \mathit{Normal}_+(0, 1)$
e. $\beta \sim \mathit{Normal}_+(0, 0.1)$
f. $\beta \sim \mathit{Normal}_+(0, 0.01)$

(A $\mathit{Normal}_+(.)$ prior can be set in `brms` by defining a lower boundary as $0$, with the argument `lb = 0`.)

What are the Bayes factors in favor of the alternative models a-f, compared to the null model?

Now carry out a standard frequentist likelihood ratio test using the `anova()` function that is used with the `lmer()` function. The commands
for doing this comparison would be:

```{r}
m_full <- lmer(log(RT) ~ c_cond +
                 (c_cond || subj) + (c_cond || item),
               df_gg05_rc)
m_null <- lmer(log(RT) ~ 1 + (c_cond||subj) + (c_cond || item),
               df_gg05_rc)
anova(m_null, m_full)
```

How do the conclusions from the Bayes factor analyses compare with the conclusion we obtain from the frequentist model comparison?

## Solution

First, let's fit all the models with the different priors. Each time we compute a Bayes factor, we save the result after making sure that the BF calculation is stable.

### Normal(0,1)
```{r}
priorNULL <- c(prior(normal(0,10), class = Intercept),
                prior(normal(0, 1), class = sd),
                prior(normal(0, 1), class = sigma),
                prior(lkj(2), class = cor))

priorN01 <- c(prior(normal(0,10), class = Intercept),
                prior(normal(0,1), class = b, coef=c_cond),
                prior(normal(0, 1), class = sd),
                prior(normal(0, 1), class = sigma),
                prior(lkj(2), class = cor))
```

```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_n01 <- brm(RT ~ c_cond + (c_cond | subj) + (c_cond | item),
                     prior = priorN01,
                     family=lognormal(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)

fit_gg05_NULL <- brm(RT ~ 1 + (c_cond | subj) + (c_cond | item),
                     prior = priorNULL,
                     family=lognormal(),
                     save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)

```

```{r message=FALSE, cache=TRUE}
bayes_factor(fit_gg05_n01,fit_gg05_NULL)
(bf_n01<-bayes_factor(fit_gg05_n01,fit_gg05_NULL)$bf)
```

### Normal(0.0.1)

```{r}
priorn0_1 <- c(prior(normal(0,10), class = Intercept),
                prior(normal(0,0.1), class = b, coef=c_cond),
                prior(normal(0, 1), class = sd),
                prior(normal(0, 1), class = sigma),
                prior(lkj(2), class = cor))
```


```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_n0_1 <- brm(RT ~ c_cond + (c_cond | subj) + (c_cond | item),
                     prior = priorn0_1,
                     family=lognormal(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)
```

```{r message=FALSE, cache=TRUE}
bayes_factor(fit_gg05_n0_1,fit_gg05_NULL)
(bf_n0_1<-bayes_factor(fit_gg05_n0_1,fit_gg05_NULL)$bf)
```

### Normal(0.01)


```{r}
priorn0_01 <- c(prior(normal(0,10), class = Intercept),
                prior(normal(0,0.01), class = b, coef=c_cond),
                prior(normal(0, 1), class = sd),
                prior(normal(0, 1), class = sigma),
                prior(lkj(2), class = cor))
```


```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_n0_01 <- brm(RT ~ c_cond + (c_cond | subj) + (c_cond | item),
                     prior = priorn0_01,
                     family=lognormal(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)
```

```{r message=FALSE, cache=TRUE}
bayes_factor(fit_gg05_n0_01,fit_gg05_NULL)
(bf_n0_01<-bayes_factor(fit_gg05_n0_01,fit_gg05_NULL)$bf)
```

### Normal+(0,1)

```{r}
priorN01tr <- c(prior(normal(0,10), class = Intercept),
                prior(normal(0,1), class = b, lb = 0),
                prior(normal(0, 1), class = sd),
                prior(normal(0, 1), class = sigma),
                prior(lkj(2), class = cor))
```

```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_n01tr <- brm(RT ~ c_cond + (c_cond | subj) + (c_cond | item),
                     prior = priorN01tr,
                     family=lognormal(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)

```

```{r message=FALSE}
bayes_factor(fit_gg05_n01tr,fit_gg05_NULL)
(bf_n01tr<-bayes_factor(fit_gg05_n01tr,fit_gg05_NULL)$bf)
```

### Normal+(0.0.1)


```{r}
priorn0_1tr <- c(prior(normal(0,10), class = Intercept),
                prior(normal(0,0.1), class = b, lb=0),
                prior(normal(0, 1), class = sd),
                prior(normal(0, 1), class = sigma),
                prior(lkj(2), class = cor))
```


```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_n0_1tr <- brm(RT ~ c_cond + (c_cond | subj) + (c_cond | item),
                     prior = priorn0_1tr,
                     family=lognormal(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)
```

```{r message=FALSE}
bayes_factor(fit_gg05_n0_1tr,fit_gg05_NULL)
(bf_n0_1tr<-bayes_factor(fit_gg05_n0_1tr,fit_gg05_NULL)$bf)
```

### Normal+(0.01) 


```{r}
priorn0_01tr <- c(prior(normal(0,10), class = Intercept),
                prior(normal(0,0.01), class = b,lb=0),
                prior(normal(0, 1), class = sd),
                prior(normal(0, 1), class = sigma),
                prior(lkj(2), class = cor))
```


```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_n0_01tr <- brm(RT ~ c_cond + (c_cond | subj) + (c_cond | item),
                     prior = priorn0_01tr,
                     family=lognormal(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)
```

```{r message=FALSE}
bayes_factor(fit_gg05_n0_01tr,fit_gg05_NULL)
(bf_n0_01tr<-bayes_factor(fit_gg05_n0_01tr,fit_gg05_NULL)$bf)
```

## Summary

```{r}
bfs<-c(bf_n01,bf_n0_1,bf_n0_01,bf_n01tr,bf_n0_1tr,bf_n0_01tr)

pr<-c("N(0,1)","N(0.0.1)","N(0.0.01)","N+(0,1)","N+(0.0.1)","N+(0.0.01)")

data.frame(priors=pr,bf=bfs)
```

What we see is that under the assumption that the prior on the target parameter is Normal(0,0.1), we have some evidence for the effect. The way I would interpret this is that under the assumption that the prior plausible range of values lie between $\pm 85$ ms with 95% probability:

```{r}
mean(df_gg05_rc$RT)
exp(6.05 + 0.2/2) - exp(6.05 - 0.2/2) 
exp(6.05 + -0.2/2) - exp(6.05 - -0.2/2) 
```

we can conclude that we have some evidence for the RC effect.

I just asked for the BFs with truncated priors, but usually we would not make such a strong assumption that we are 100% sure that the effect is positive (we can never really be sure). But in theory there could be situations where a truncated prior is justifiable, so I included an example. of that here.

By contrast, the frequentist analysis gives a somewhat misleading conclusion: strong evidence for the effect. For more discussion, see:

- Shravan Vasishth and Andrew Gelman. How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis. Linguistics, 59:1311--1342, 2021
- Shravan Vasishth, Himanshu Yadav, Daniel Schad, and Bruno Nicenboim. Sample size determination for Bayesian hierarchical models commonly used in psycholinguistics. Computational Brain and Behavior, 2022.
- Shravan Vasishth. Some right ways to analyze (psycho)linguistic data. Annual Review of Linguistics, 9:273–291, 2023.

# Exercise 3: In the Grodner and Gibson 2005 data, in question-response accuracies, is there evidence for the claim that sentences with subject relative clauses are easier to comprehend?

Assume here that for the effect of RC on question accuracy, $\beta \sim \mathit{Normal}(0, 0.1)$ is a reasonable prior, and that for all the variance components, the same prior, $\tau \sim \mathit{Normal}_{+}(0, 1)$, is a reasonable prior.

Consider the question response accuracy of the data of Experiment 1 of Grodner and Gibson 2005.

a. Compare a model that assumes that RC type affects question accuracy on the population level and with the effect varying by-subjects and by-items with *a null model* that assumes that there is no population-level effect present.

b. Compare a model that assumes that RC type affects question accuracy on the population level, where the effect varies by-subjects and by-items with *another null model* that assumes that there is no population-level or group-level effect present, that is, that there is no overall effect and there are no by-subject or by-item effects. What's the meaning of the results of the Bayes factor analysis?


## Solution

Descriptively, the mean accuracy for the object gap condition is a bit higher (surprising! It should have been lower). 

```{r}
head(df_gg05_rc)
with(df_gg05_rc,tapply(qcorrect,condition,mean))
```


### (a)

```{r}
priors<-c(prior(normal(0,10), class = Intercept),
                prior(normal(0,0.1), class = b, coef=c_cond),
                prior(normal(0, 1), class = sd),
                prior(lkj(2), class = cor))

priorsNULL <- c(prior(normal(0,10), class = Intercept),
                #prior(normal(0,0.1), class = b, coef=c_cond),
                prior(normal(0, 1), class = sd),
                prior(lkj(2), class = cor))
```

```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_acc <- brm(qcorrect ~ c_cond + (c_cond | subj) + 
                      (c_cond | item),
                     prior = priors,
                     family=bernoulli(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)

fit_gg05_accNULL <- brm(qcorrect ~ c_cond + (c_cond | subj) + 
                      (c_cond | item),
                     prior = priorsNULL,
                     family=bernoulli(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)
```

```{r}
bayes_factor(fit_gg05_acc,fit_gg05_accNULL)
bayes_factor(fit_gg05_acc,fit_gg05_accNULL)
```

Inconclusive.

### (b)

```{r}
priorsNULL2 <- c(prior(normal(0,10), class = Intercept)
                #prior(normal(0,0.1), class = b, coef=c_cond),
                # prior(normal(0, 1), class = sd),
                #prior(lkj(2), class = cor)
                )
```

```{r message = FALSE, results = "hide", cache=TRUE}
fit_gg05_accNULL2 <- brm(qcorrect ~ 1,
                     prior = priorsNULL2,
                     family=bernoulli(),
                    save_pars = save_pars(all = TRUE),
                     warmup=2000,
                     iter=20000,
                     data = df_gg05_rc)
```

```{r warning=FALSE}
bayes_factor(fit_gg05_acc,fit_gg05_accNULL2)
bayes_factor(fit_gg05_acc,fit_gg05_accNULL2)
```

The massive BF is arising because our null model is overly simple; it's easy for the full model to achieve a much higher marginal likelihood. This isn't a comparison one would ever do in practice.

