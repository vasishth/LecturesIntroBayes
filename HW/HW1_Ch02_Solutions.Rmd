---
title: "HW 1 Solutions"
author: "Shravan Vasishth"
date: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercise 1

The French mathematician Pierre-Simon Laplace (1749-1827) was the first person to show definitively that the proportion of female births in the French population was less then $0.5$, in the late 18th century, using a Bayesian analysis based on a uniform prior distribution. 

Suppose you were doing a similar analysis but you had more definite prior beliefs about the ratio of male to female births. In particular, if $\theta$ represents the proportion of female births in a given population, you are willing to place a Beta(100,100) prior distribution on $\theta$.

- Show that this means you are more than 95\% sure that $\theta$ is between $0.4$ and $0.6$, although you are ambivalent as to whether it is greater or less than $0.5$.

- Now you observe that out of a random sample of $1,000$ births, $511$ are boys. What is your posterior probability that $\theta> 0.5$?


## Solution to exercise 1

The probability that $\theta$ lies between 0.6 and 0.4:

```{r}
pbeta(0.6,shape1=100,shape2=100)-pbeta(0.4,shape1=100,shape2=100)
```

The data: n=1000 people, 511 boys. (Assuming a binary gender here because back then they only recognized two genders.) So, k = 1000-511 = 489 girls.

The posterior will be Beta(a=100+489,b=100+511). (See lecture slides and book if it is not clear why). That means that the proportion of female births is:

```{r}
a<-100+489
b<-100+1000-489
## The mean of a Beta distribution:
a/(a+b)
```

The probability that $\theta$ is larger than 0.5:


```{r}
pbeta(0.5,shape1=100+489,shape2=100+1000-489,lower.tail=FALSE)

```


# Exercise 2

Suppose we are modeling the egocentricity of a person as measured by their speech. A researcher theorizes that one can get an idea of a speaker's egocentricity by measuring the number of times they produce a first-person pronoun in a sentence per day (e.g., starting a sentence with "I", or using a word like "me", "my', etc., anywhere in a sentence).

The number of times $x$ that such a pronoun is used can be modeled by a Poisson distribution. 

We are going to first define a Gamma prior on the $\lambda$ parameter (representing the rate of occurrences of the pronouns) in the Poisson distribution. We have prior data suggesting that the mean number of occurrences of a first person personal pronoun in a population is 75 and the variance is 185.  Using the mean and variance, figure out the hyperparameters of the Gamma prior, i.e., find out what a and b will be in Gamma(a,b). Hint: $a/b$ is the mean of a Gamma distribution, and $a/b^2$ is the variance.
Once you have figured out the Gamma hyperparameters, plot the Gamma distribution representing the priors.

Now suppose that the observed number of pronoun utterances per day over four days is 
200,87,99,121. What is the posterior distribution of the $\lambda$ parameter? This distribution will be a Gamma distribution with updated hyperparameters $a^*$ and $b^*$. Your task is to figure out what these updated values $a^*$ and $b^*$ are.

Now, suppose that you get additional data for *two* further days, and the number of produced utterances over the two days taken together is 300. [Hint: this means that the data 300 is from n=2 days].

i. Use the posterior you just obtained (Gamma($a^*$,$b^*$)) as a prior for this new data, and write down the posterior distribution for $\lambda$ as a Gamma($a^{**}$,$b^{**}$) distribution, where $a^{**}$,$b^{**}$ are the updated hyperparameters. 

ii. Now, start with your original prior Gamma(a,b), and use all the data available (200,87,99,121,300) and derive the posterior distribution of $\lambda$, expressed as a Gamma distribution with some hyperparameters $a^{***}$ and $b^{***}$. (Hint: think carefully about what n is. Is it 5 or 6?)  

Is there any difference in the posterior between the above two analyses (i) and (ii)? 



## Solution to exercise 2

The number of times $x$ that the word of interest is uttered in one day can be modeled by a Poisson distribution:

\begin{equation}
f(x\mid \lambda) = \frac{\exp(-\lambda) \lambda^x}{x!}
\end{equation}

where the rate $\lambda$ is unknown, and the numbers of utterances of the target word on each day are independent given $\lambda$.

We are told that the prior mean of $\lambda$ is 75 and prior variance for $\lambda$  is 185. 

In order to visualize the prior, we first fit a Gamma density prior for $\lambda$ based on the above information. 

As explained in the assignment (the hint), we know that for a Gamma density with hyperparameters a, b, the mean is  $\frac{a}{b}$ and the variance is
$\frac{a}{b^2}$.
Since we are given values for the mean and variance, we can solve for a,b, which gives us the Gamma density. 

If $\frac{a}{b}=75$ and $\frac{a}{b^2}=185$, it follows that

- $a = 75\times b$
- $a=185\times b^2$

So, $185 \times b^2 = 75 \times b$. Simplifying:
$185 \times b = 75$.  So $b=\frac{75}{185}$.

This means that $a= 75\times\frac{75}{185}=\frac{5625}{185}$.

The Gamma distribution for the prior on $\lambda$ is as shown below.

\begin{equation}
\lambda \sim Gamma(\frac{5625}{185},\frac{75}{185})
\end{equation}

```{r,fig.height=4,fig.cap="\\label{fig1}The Gamma prior for the parameter lambda."}
a<-5625/185
b<-75/185

lambda<-0:200
plot(lambda,dgamma(lambda,shape=a,rate=b),type="l",lty=1,main="Gamma prior",ylab="density",cex.lab=2,cex.main=2,cex.axis=2)
```

Given that 

\begin{equation}
\hbox{Posterior} \propto \hbox{Prior}~\hbox{Likelihood}
\end{equation}

and given that the likelihood is:

\begin{equation}
\begin{split}
L(\mathbf{x}\mid \lambda) =& \prod_{i=1}^n \frac{\exp(-\lambda) \lambda^{x_i}}{x_i!}\\
          =& \frac{\exp(-n\lambda) \lambda^{\sum_i^{n} x_i}}{\prod_{i=1}^n x_i!}\\
\end{split}          
\end{equation}

we can compute the posterior as follows:

\begin{equation}
\hbox{Posterior} = \left[\frac{\exp(-n\lambda) \lambda^{\sum_i^{n} x_i}}{\prod_{i=1}^n x_i!}\right]
\left[ \frac{b^a \lambda^{a-1}\exp(-b\lambda)}{\Gamma(a)} \right]
\end{equation}

Disregarding the terms $x!,\Gamma(a), b^a$,  which do not involve $\lambda$, we have

\begin{equation}
\begin{split}
\hbox{Posterior} \propto &  \exp(-n\lambda)  \lambda^{\sum_i^{n} x_i} \lambda^{a-1}\exp(-b\lambda)\\
=& \lambda^{a-1+\sum_i^{n} x_i} \exp(-\lambda (b+n))
\end{split}
\end{equation}


First, note that the Gamma distribution in general is $Gamma(a,b) \propto \lambda^{a-1} \exp(-\lambda b)$. So it's enough to state the above as a Gamma distribution with some parameters $a^*$, $b^*$.

If we equate $a^{*}-1=a-1+\sum_i^{n} x_i$ and $b^{*} = b+n$, we can rewrite the above as:

\begin{equation}
\lambda^{a^{*}-1} \exp(-\lambda b^{*})
\end{equation}

This means that $a^{*}=a+\sum_i^{n} x_i$ and $b^{*}=b+n$.


Thus, the posterior has the form of  a Gamma distribution with parameters 
$a^{*}=a+\sum_i^{n} x_i, b^{*}=b+n$. 

The data:

```{r}
x<-c(200,87,99,121)
n<-length(x)
sumx<-sum(x)
sumx
```


The posterior's hyperparameters are:

```{r}
(astar<-a+sumx)
(bstar<-b+n)
```

The posterior for $\lambda$ is $Gamma(`r round(astar,2)`, `r round(bstar,2)`)$.

The new data we get is 300 utterances of the target word, but these are over two days. So our n is:

```{r}
n<-2
```

We are adding two, not one, to our number of data points because our unit of measurement was number of utterances per day. If we get 300 utterances over two days, we have to use 2 as the sample size.

The summed values of the new observed utterances is:

```{r}
sumx<-300
```

The new posterior is:

```{r}
(astarstar<-astar+sumx)
(bstarstar<-bstar+n)
```

The updated posterior (rounded to two decimal places again) is $Gamma(`r round(astarstar,2)`, `r round(bstarstar,2)`)$.



Notice an important point that I mentioned in class relating to a question on updating the posterior given new data: the above calculation is exactly the same as pooling all the data, i.e,   200,87,99,121, 300, setting n=6, and then figuring out the posterior using the original prior that we had for the $\lambda$ parameter.

```{r}
x<-c(200,87,99,121,300)
n<-length(x) + 1 # the 300 is for two days
sumx<-sum(x)
sumx
```

```{r}
(astarstarstar<-a+sumx)
(bstarstarstar<-b+n)
```

Notice that the two sets of hyperparameters are the same:

Generally, it is the case that successively using the posterior from a previous study as a prior in a future study is the same thing as analyzing all the data from the previous and current study together, using the prior from the original study.

```{r}
astarstar; bstarstar
astarstarstar; bstarstarstar
```

# Bonus Exercise 3

Suppose that 1 in 1000 people in a population is expected to get HIV. Suppose a test is administered on a suspected HIV case, where the test has a true positive rate (the proportion of positives that are actually HIV positive) of 95\%  and true negative rate (the proportion of negatives are actually HIV negative) 98\%. 
Use Bayes' theorem to find out the probability that a patient testing positive actually has HIV.

## Solution to bonus exercise 3

Need to find: Prob(HasHIV | TestedPositive)

We know:

- Prob(HasHIV) = 1/1000. Therefore, Prob($\neg$HasHIV) = 999/1000. 
- Prob(TestedPositive | HasHIV) = 0.95
- Prob($\neg$TestedPositive | $\neg$ HasHIV) = 0.98. This implies that Prob(TestedPositive | $\neg$ HasHIV) = 1-0.98 = 0.02

Notice that, from the law of total probability (section 1.3 of book draft), 


$$Prob(TestPos) = Prob(TestPos|HasHIV)Prob(HasHIV) + Prob(TestPos|\neg HasHIV)Prob(\neg HasHIV)$$

Here, TestPos=TestedPositive.

So, 

$$Prob(TestedPositive) = 0.95 \times 1/1000 + 
0.02\times 999/1000 = `r round(0.95*(1/1000) + 0.02 * (999/1000),3)`$$

Now, Bayes' rule is:

$$Prob(A|B) = \frac{Prob(B|A)Prob(A)}{Prob(B)}$$

Let A=HasHIV, B=TestedPositive. We can rewrite the above in terms of our research question:

$$Prob(HasHIV|TestedPositive) = \frac{Prob(TestedPositive|HasHIV)Prob(HasHIV)}{Prob(TestedPositive)}$$

Plugging in the probabilities:

$$Prob(HasHIV|TestedPositive) = \frac{0.95\times (1/1000)}{0.03} = `r round((0.95*(1/1000))/0.021,3)`$$

So, the probability of having HIV given that one has tested positive in this situation is, surprisingly, `r round((0.95*(1/1000))/0.021,3)`.

This has to do with the low base rate of HIV (1/1000). Suppose that HIV was rampant, and the probability of having HIV was 999/1000 (obviously a ridiculously unrealistic example, but just to illustrate the effect of base rates).

Now, 

$$Prob(TestedPositive) = 0.95 \times 999/1000 + 
0.02\times 1/1000 = `r round(0.95*(999/1000) + 0.02 * (1/1000),3)`$$

This implies that

$$Prob(HasHIV|TestedPositive) = \frac{0.95\times (999/1000)}{0.949} = `r round((0.95*(999/1000))/0.949,3)`$$

Now, the probability of having HIV given that one has tested positive is 1.

In other words, the base rate (the prior probability of something happening) matters a lot in interpreting the posterior probability given some data. This makes intuitive sense: if some event is highly unlikely a priori, then seeing any evidence that event has happened should not shift your prior belief much that the event happened. By contrast, if something is highly likely to happen a priori, then finding evidence that the event happened should lead you to believe that that event actually did happen.

One practical example of this is the infamous Bem paper I mentioned in class:

Bem, D. J. (2011). Feeling the future: experimental evidence for anomalous retroactive influences on cognition and affect. Journal of personality and social psychology, 100(3), 407.

It doesn't really matter that he got significant effects of pre-cognition; the prior probability of people being able to tell the future accurately is near 0. So any evidence he reports of pre-cognition doesn't shift my belief at all.


Here is another example, looking at frequentist null hypothesis testing and how it shifts our beliefs about the null hypothesis:


Daniel J. Schad and Shravan Vasishth. The posterior probability of a null hypothesis given a statistically significant result. Quantitative Methods for Psychology, 2022. 

https://danielschad.shinyapps.io/probnull/

