---
title: "HW 3 Solutions"
author: "Shravan Vasishth"
date: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("index.R")
```

# Exercise 1

We need to create a new column first (there are other ways of doing this): 

```{r}
data("df_powerpose")
head(df_powerpose)
df_powerpose <- mutate(df_powerpose, change = testm2 - testm1)
```

```{r, message = FALSE, results = "hide", cache=TRUE}
## use default priors
fit_powerpose <- brm(change ~ hptreat,data =  df_powerpose)
```

```{r}
fit_powerpose
```

There does seem to be a lower testosterone level in the low power pose condition compared to the high power pose condition; this is consistent with the hypothesis. However, the credible intervals are very wide, and it would be hard to interpret the average effect as confirming the hypothesis. A Bayes factor analysis would be needed to claim evidence for an effect, but it is unlikely that such evidence would be forthcoming (the reader should check this after reading the Bayes factor chapter).

If we want to know which priors `brms` used by default, we do the following:

```{r}
prior_summary(fit_powerpose)
```

We can inspect this even before running the model in the following way.

```{r}
get_prior(change ~ hptreat, df_powerpose)
```


# Exercise 2

## (a) Our priors for this experiment were quite arbitrary. How do the prior predictive distributions look like? Do they make sense?

We first create a centered version of load:

```{r}
data("df_pupil")
df_pupil <- df_pupil %>%
  mutate(c_load = load - mean(load))
```

We first sample from the priors only:

```{r, message = FALSE, results = "hide", cache=TRUE}
fit_pupil_prior <- brm(p_size ~ 1 + c_load,
  data = df_pupil,
  family = gaussian(),
  prior = c(
    prior(normal(1000, 500), class = Intercept),
    prior(normal(0, 1000), class = sigma),
    prior(normal(0, 100), class = b, coef = c_load)
  ),
  sample_prior = "only",
  control = list(adapt_delta = .99)
)
```

```{r}
pp_check(fit_pupil_prior, ndraws = 100)
```


They don't look too good, we are assuming that pupil sizes can be even negative!

In fact the prior distribution of minimum values shows that we are putting too much prior probability mass on negative values of pupil size:

```{r}
pp_check(fit_pupil_prior,
  type = "stat",
  stat = "min",
  binwidth = 100
) +
  coord_cartesian(xlim = c(-5000, 5000))
```

As an extra step, let's look at our choice for $\sigma$. Our choice of prior for $\sigma$ might have been too wide leading to too much variation in the prior predictive distribution.

The following seems like a more reasonable prior:

\begin{equation}
\sigma \sim Normal_+(100, 50)
\end{equation}

```{r fitpupilsigmaprior, message = FALSE, results = "hide", cache=TRUE}
fit_pupil_sigma2_prior <- brm(p_size ~ 1 + c_load,
  data = df_pupil,
  family = gaussian(),
  prior = c(
    prior(normal(1000, 500), class = Intercept),
    prior(normal(100, 50), class = sigma),
    prior(normal(0, 100), class = b, coef = c_load)
  ),
  sample_prior = "only",
)
```

```{r}
pp_check(fit_pupil_sigma2_prior,
  type = "stat",
  stat = "min",
  binwidth = 100
) +
  coord_cartesian(xlim = c(-5000, 5000))
```

It's still not perfect, but we have reduced the probability mass of negative pupil sizes. We could continue tuning our priors and using our prior knowledge, this also might lead to a change in the likelihood. For now, we stop here.

## (b) Is our posterior distribution sensitive to the priors that we selected? Perform a sensitivity analysis to find out whether the posterior is affected by our choice of prior for  $\sigma$.


We'll check now if our estimates change when we select a more informative prior for $\sigma$:

\begin{equation}
\sigma \sim Normal_+(100, 50)
\end{equation}


```{r fitpupilsigma, message = FALSE, results = "hide", cache=TRUE}
fit_pupil_sigma2 <- brm(p_size ~ 1 + c_load,
  data = df_pupil,
  family = gaussian(),
  prior = c(
    prior(normal(1000, 500), class = Intercept),
    prior(normal(100, 50), class = sigma),
    prior(normal(0, 100), class = b, coef = c_load)
  )
)
```


```{r}
fit_pupil_sigma2
```

We see that our estimates remain virtually identical. One can continue and check other potential priors for $\sigma$.


## (c) Our data set includes also a column that indicates the trial number. Could it be that trial has also an effect on the pupil size? As in lm, we indicate another main effect with a + sign. How would you communicate the new results?

As we did with pupil size, we first create a centered version of trial:
```{r}
df_pupil <- df_pupil %>% mutate(c_trial = trial - mean(trial))
```

We'll fit a model with the following likelihood:

\begin{equation}
p\_size_n \sim Normal(\alpha + c\_load_n \cdot \beta_1 + c\_trial_n \cdot \beta_2, \sigma )
\end{equation}

The formula will look like this ` 1 + c_load + c_trial`. We can see the new priors that the model needs by doing the following:

```{r}
get_prior(p_size ~ 1 + c_load + c_trial,
  data = df_pupil,
  family = gaussian()
)
```

For simplicity, we assign the same prior distribution to both $\beta$ parameters.

```{r fitpupiltrial2, message = FALSE, results = "hide"}
fit_pupil_trial <- brm(p_size ~ 1 + c_load + c_trial,
  data = df_pupil,
  family = gaussian(),
  prior = c(
    prior(normal(1000, 500), class = Intercept),
    prior(normal(0, 1000), class = sigma),
    prior(normal(0, 100), class = b)
  )
)
```

```{r}
fit_pupil_trial
```
```{r, echo = FALSE}
load <- posterior_summary(fit_pupil_trial)["b_c_load", c("Estimate", "Q2.5", "Q97.5")] %>%
  round(2)

trial <- posterior_summary(fit_pupil_trial)["b_c_trial", c("Estimate", "Q2.5", "Q97.5")] %>% round(2)
```

The summary of the posterior tells us that the most likely values of the effect of load will be around the mean of the posterior, `r load[1]`, and we can be 95% certain that the true value of the effect of load  (given the model and the data) lies between `r load[2]` and `r load[3]`. As before, as load increases the pupil size increases. The mean of the posterior for the effect of trial will be `r trial[1]`, with a 95% credible interval of $[`r trial[2]`, `r trial[3]`]$; this is telling us that as the trials proceed further, the pupil size is reduced.


# Exercise 3

## (a) Estimate the slowdown in milliseconds between the last two times the subject pressed the space bar in the experiment.

We look at what happens between the last two trials in milliseconds in the following way:

```{r,  message = FALSE, echo = FALSE, results = "hide"}
df_spacebar <- df_spacebar %>%
  mutate(c_trial = trial - mean(trial))

fit_press_trial <- brm(t ~ 1 + c_trial,
  data = df_spacebar,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma),
    prior(normal(0, .01), class = b, coef = c_trial)
  )
)
```

```{r}
alpha_samples <- as_draws_df(fit_press_trial)$b_Intercept
beta_samples <- as_draws_df(fit_press_trial)$b_c_trial
last_trial <- df_spacebar$c_trial %>% max()
effect_end_ms <- exp(alpha_samples + last_trial * beta_samples) -
  exp(alpha_samples + (last_trial - 1) * beta_samples)
c(mean = mean(effect_end_ms), quantile(effect_end_ms, c(.025, .975)))
```

## (b)

We first create the new predictors:

```{r,  message = FALSE}
df_spacebar <- df_spacebar %>%
  mutate(
    c_log_trial = log(trial) - mean(log(trial)),
    c_sqrt_trial = sqrt(trial) - mean(sqrt(trial))
  )
```
We start with log trial. The slope ($\beta$) now represents the change in log response times when we move from the centered log transformed trial  (`c_log_trial`)  0 to 1, which corresponds to going from trial  `r exp(0)` to trial  "`r exp(1)`" (because $\exp(0)= `r exp(0)`$ and $\exp(1) = `r exp(1)`$). However the scale is not linear, and thus when we move from log transformed trials 2 to 3, it represents going from trial "`r exp(2)`" to trial  "`r exp(3)`" (because $\exp(2)= `r exp(2)`$ and $\exp(3) = `r exp(3)`$). This means that our prior for $\beta$ from section \@ref(sec-trial) might be too restrictive, one unit for the new $\beta$  can be much more than one trial. We change the prior for $\beta$ in the next model to $Normal(0, 1)$:

```{r,  message = FALSE, results = "hide"}
fit_press_log_trial <- brm(t ~ 1 + c_log_trial,
  data = df_spacebar,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma),
    prior(normal(0, 1), class = b, coef = c_log_trial)
  )
)
```

We will look at the effect in  milliseconds for the middle of the experiment. Thus we need to know `c_log_trial` at the middle and one trial before in our experiment. (Notice that it won't be 0 and -1 as before).

```{r}
middle_log <- df_spacebar %>%
  filter(c_trial == 0) %>%
  pull(c_log_trial)
## in base R this would be  <- df_spacebar[c_trial == 0]$c_log_trial
middle_m1_log <- df_spacebar %>%
  filter(c_trial == -1) %>%
  pull(c_log_trial)
```

```{r}
alpha_samples <- as_draws_df(fit_press_log_trial)$b_Intercept
beta_samples <- as_draws_df(fit_press_log_trial)$b_c_log_trial
effect_middle_ms <- exp(alpha_samples + middle_log * beta_samples) -
  exp(alpha_samples + middle_m1_log * beta_samples)
c(mean = mean(effect_middle_ms), quantile(effect_middle_ms, c(.025, .975)))
```

We can do the same for square-root-transformed trials

```{r,  message = FALSE, results = "hide"}
fit_press_sqrt_trial <- brm(t ~ 1 + c_sqrt_trial,
  data = df_spacebar,
  family = lognormal(),
  prior = c(
    prior(normal(6, 1.5), class = Intercept),
    prior(normal(0, 1), class = sigma),
    prior(normal(0, 1), class = b, coef = c_sqrt_trial)
  )
)
```

```{r}
middle_sqrt <- df_spacebar %>%
  filter(c_trial == 0) %>%
  pull(c_sqrt_trial)
middle_m1_sqrt <- df_spacebar %>%
  filter(c_trial == -1) %>%
  pull(c_sqrt_trial)
```

```{r}
alpha_samples <- as_draws_df(fit_press_sqrt_trial)$b_Intercept
beta_samples <- as_draws_df(fit_press_sqrt_trial)$b_c_sqrt_trial
effect_middle_ms <- exp(alpha_samples + middle_sqrt * beta_samples) -
  exp(alpha_samples + middle_m1_sqrt * beta_samples)
c(mean = mean(effect_middle_ms), quantile(effect_middle_ms, c(.025, .975)))
```


Let's also look at the descriptive adequacy of these three models. You cannot easily combine models with bayesplot (which in turn is what `brms` uses to plot). See https://github.com/stan-dev/bayesplot/issues/232

This requires a bit of R knowledge. (Another alternative is to just have 3 plots).

```{r}
# I use imap to iterate across the fit of the three models,
# and create a long df with predictions of the three models
df_noreading_data_pred <-
  imap_dfr(
    list(raw = fit_press_trial, log = fit_press_log_trial, sqrt = fit_press_sqrt_trial),
    function(fit, model) {
      posterior_predict(fit, ndraws = 1000) %>%
        array_branch(margin = 1) %>%
        map_dfr(function(yrep_iter) {
          df_spacebar %>%
            mutate(
              rt = yrep_iter,
              model = model
            )
        }, .id = "iter") %>%
        mutate(iter = as.numeric(iter))
    }
  )
```

We see that for the range of trials that we have in our experiment, the three models make very similar predictions:

```{r ,  message = FALSE}
df_noreading_pred_summary <- df_noreading_data_pred %>%
  # I create 12 intervals of trials
  mutate(inter = cut(trial, breaks = 12)) %>%
  group_by(model, iter, inter) %>%
  summarize(rt = mean(rt))

# observed means:
df_noreading_summary <- df_spacebar %>%
  mutate(inter = cut(trial, breaks = 12)) %>%
  group_by(inter) %>%
  summarize(rt = mean(t))

ggplot(df_noreading_pred_summary, aes(rt)) +
  geom_histogram(alpha = 0.5, aes(fill = model), position = "identity") +
  geom_vline(aes(xintercept = rt), data = df_noreading_summary) +
  xlab("button-tapping times") +
  facet_wrap(inter ~ .)
```



# Exercise 4

```{r, message = FALSE, results = "hide",cache=TRUE}
data("df_recall")
df_recall <- df_recall %>%
  mutate(c_set_size = set_size - mean(set_size),
         c_tested = tested - mean(tested))

fit_recall <- brm(correct ~ 1 + c_set_size + c_tested,
  data = df_recall,
  family = bernoulli(link = logit),
  prior = c(
    prior(normal(0, 1.5), class = Intercept),
    #same prior for both parameters
    prior(normal(0, .1), class = b)
  )
)
```

```{r}
fit_recall
```


# Exercise 5

```{r}
data("df_red")
head(df_red)
```

We fit three different models. We use similar priors as before.


```{r, message = FALSE, results = "hide",cache=TRUE}
fit_red <- brm(red ~ 1 + risk,
  data = df_red,
  family = bernoulli(link = logit),
  prior = c(
    prior(normal(0, 1.5), class = Intercept),
    prior(normal(0, 0.1), class = b, coef= risk)
  )
)
```

```{r}
fit_red
```

```{r, message = FALSE, results = "hide", cache=TRUE}
fit_pink <- brm(pink ~ 1 + risk,
  data = df_red,
  family = bernoulli(link = logit),
  prior = c(
    prior(normal(0, 1.5), class = Intercept),
    prior(normal(0, 0.1), class = b)
  )
)
```

```{r}
fit_pink
```

```{r, message = FALSE, results = "hide", cache=TRUE}
fit_redorpink <- brm(redorpink ~ 1 + risk,
  data = df_red,
  family = bernoulli(link = logit),
  prior = c(
    prior(normal(0, 1.5), class = Intercept),
    prior(normal(0, .1), class = b)
  )
)
```

```{r}
fit_redorpink
```

The slope parameter's posterior in the models doesn't seem to be consistent with the theory. However, to establish whether the effect "exists" or not, one would need to do a formal hypothesis test (Bayes factor).
